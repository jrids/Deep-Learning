{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Image Classification using CNN"
      ],
      "metadata": {
        "id": "tPM2wtBQYMlx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Background\n",
        "The effort so far has pretty much  been on understanding the classical machine learning algorithms and ofcourse, bringing them to life with some nicer and neater implementations. Give yourself a pat on the back......great job thus far! With your favourite story teller Prajwal Singh (kudos to him) taking a break from his story, we invite you to take a dive into some deep learning stuff. But don't worry, we assure you that he will be back with his story soon.<br> <br> We now move towards understanding and implementing the Convolutional Neural Networks (CNN). Just to give you a background, the advent of the idea of CNNs dates back to 1959, when David Hubel and Torsten Wiesel described “simple cells” and “complex cells” in the human visual cortex. They proposed in 1962 that complex cells achieve spatial invariance by “summing” the output of several simple cells that all prefer the same orientation but different receptive fields (e.g. bottom, middle, or top of an image). By collecting information from a bunch of simple cell minions, the complex cells can respond to horizontal bars that occur anywhere. This concept — that simple detectors can be “summed” to create more complex detectors — is found throughout the human visual system, and is also the fundamental basis of convolution neural network models. <br> <br> Inspired by this idea, networks like LENet and AlexNet were introduced to mark the foundation of modern convolutional neural networks. Ofcourse, after that we witnessed a plenty of new architectures (VGGNet, ResNet, etc), so many in number that the space will get flooded if we try to list them all down."
      ],
      "metadata": {
        "id": "R7qLa7SnU1eC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Image Classification\n",
        "\n",
        "Well...you know it already, but just for the sake of completeness - <i>Image classification is the task of assigning a label to an input image from a fixed set of classes.</i> For instance, in the following figure, an image classification model takes in an input image from a dataset and assigns a label to it from the given classes in the dataset. Clearly, that's what you will be doing!<br><br><br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<img src='https://rishabhjain.xyz/ml-class-project/public/images/cat_mod.png' type='image'>\n"
      ],
      "metadata": {
        "id": "RT8BYyonWvJD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Objective\n",
        "\n",
        "Let's implement [VGG19](https://arxiv.org/pdf/1409.1556.pdf) and\n",
        "and [ResNet](https://arxiv.org/pdf/1512.03385.pdf) on the CIFAR10 dataset for image classification and also compare their performance with a simple n-layer feedforward neural network (as you must have done in the last assignment. However, we will do so under two experimental settings.\n",
        "\n",
        "**Dataset**\n",
        "\n",
        "We will again use the CIFAR-10 dataset. This dataset consists of 60,000 RGB images of size 32x32. The images belong to objects of 10 classes such as frogs, horses, ships, trucks etc. The dataset is divided into 50,000 training images and 10,000 testing images. Among the training images, we used 49,000 images for training and 1000 images for validation. <img src='https://rishabhjain.xyz/ml-class-project/public/images/cifar-10.png' type='image'>\n",
        "\n",
        "**Experimental Settings**\n",
        "\n",
        "* **Supervised Learning**: Since we have labels for all images, we will use supervised learning to solve this task. Here, we will use all 49,000 training samples with their labels for supervised learning.\n",
        "\n",
        "* **Semi-supervised Learning**: It comes handy when we have a dataset that is partially labelled or we want to use a pool of unlabeled data for training. To simulate this setting, we will split the CIFAR-10 training data into 2 parts, 5,000 images which have labels and the rest 44,000 images which are treated as unlabeled images. We shall use <i>self-training</i> as a wrapper on supervised learning technique for semi-supervised learning.\n",
        "\n",
        "\n",
        "**What is Self Training?**\n",
        "\n",
        "Self-Training is a technique which can be applied to any supervised learning algorithm to train it in a semi-supervised fashion. The algorithm first uses the labeled data to train the model. After training it for few iterations, some part of unlabled data is labeled using the trained model and is added to the set of labeled training data for the next iteration."
      ],
      "metadata": {
        "id": "GOUs3Eg2e8gX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implementation Details\n",
        "\n",
        "**Libraries**\n",
        "\n",
        "This time we give you freedom to choose between [Pytorch](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) and [Tensorflow](https://www.tensorflow.org/tutorials/images/cnn). Go through these links to get acquainted yourself with the frameworks and decide on your choice. Make sure you follow more or less the same format while coding in either of the two deep learning libraries.\n",
        "\n",
        "**Data Augmentation**\n",
        "\n",
        "It is the process of preprocessing the data before feeding it to the model. It increases the diversity of the data which allows the model to generalize well and results in higher accuracy. We will primarily do the following three types of modification on the dataset.\n",
        "\n",
        "* Random Crop: Add sufficient padding to the original image and then randomly crop to create a new image.\n",
        "* Random Horizontal Flip: The original image is randomly flipped horizontally to produce new image.\n",
        "* Resizing the Original Image: The original image will be resized to a different shape. Some deep neural networks require the image to be of sufficient size due to multiple down-sampling layers in the architecture.\n",
        "\n",
        "**Hyperparameter Optimization**\n",
        "\n",
        "The accuracy of any particular model depends on the careful selection of the optimizer and the parameters that optimizer depends on, such as learning rate, momentum for Stochastic Gradient Descent, weight decays, and batch size.\n",
        "\n",
        "To find the optimum value of these hyperparameters, vary these hyperparameters over a certain range and track the accuracy of the model on the validation dataset.\n",
        "\n",
        "For instance, the following figure shows the performance of a sample ResNet model with different learning rates for Adam optimizer. We can clearly see that model performs best at the learning rate of 0.001. Other learning rates are either too small or too large for the model to converge.\n",
        "\n",
        "<img src='https://rishabhjain.xyz/ml-class-project/public/images/supervised_model_performance/learningRateTuning.svg' type='image'>\n"
      ],
      "metadata": {
        "id": "TuQrYsy8gIaC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deliverables\n",
        "\n",
        "* Perform image classifcation over CIFAR10 dataset using VGG19, ResNet18, and an n-layer MLP. Compare their performances by reporting the confusion matrices.\n",
        "\n",
        "* Compare the model performance under the two settings (described above) and report your observations. Note: Do include the optimum values for the hyperparameters and show the plots (as above) justifying your choice.\n",
        "\n",
        "* Include the training plots (Train loss vs Epochs) for each of the trained models."
      ],
      "metadata": {
        "id": "5Kq_bAYHl5-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n"
      ],
      "metadata": {
        "id": "xtcGGAvgzP1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MLP,self).__init__()\n",
        "    self.lin1 = nn.Linear(64 * 64 * 3, 128)\n",
        "    self.lin2 = nn.Linear(128, 256)\n",
        "    self.lin3 = nn.Linear(256, 512)\n",
        "    self.lin4 = nn.Linear(512, 256)\n",
        "    self.lin5 = nn.Linear(256, 128)\n",
        "    self.lin6 = nn.Linear(128, 64)\n",
        "    self.lin7 = nn.Linear(64, 10)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = F.relu(self.lin1(x))\n",
        "    x = F.relu(self.lin2(x))\n",
        "    x = F.relu(self.lin3(x))\n",
        "    x = F.relu(self.lin4(x))\n",
        "    x = F.relu(self.lin5(x))\n",
        "    x = F.relu(self.lin6(x))\n",
        "    x = self.lin7(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "dSH7yryaykJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_train = transforms.Compose(\n",
        "    [transforms.RandomHorizontalFlip(),\n",
        "     transforms.RandomCrop(size=32, padding=4),\n",
        "     transforms.Resize((64,64)),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform_test = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Resize((64,64)),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "print(len(trainloader))\n",
        "print(len(testloader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qf5l5MDSzLck",
        "outputId": "b4026bee-5c29-48a6-c3a2-5471c4d12a29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "3125\n",
            "625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "net = MLP().to(device)\n",
        "print(net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_ruyu8dzi29",
        "outputId": "e4d30d1a-6d3c-491d-94cf-a995044a280b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (lin1): Linear(in_features=12288, out_features=128, bias=True)\n",
            "  (lin2): Linear(in_features=128, out_features=256, bias=True)\n",
            "  (lin3): Linear(in_features=256, out_features=512, bias=True)\n",
            "  (lin4): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (lin5): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (lin6): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (lin7): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "bIlH7_GOz0OO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "epoch_losses = [] # using this to record the training loss so that we can plot it against the epoch\n",
        "net.train()\n",
        "for epoch in range(10):\n",
        "    print(\"Epoch:\", epoch)\n",
        "    print(\"-\"*100)\n",
        "    running_loss = 0.0\n",
        "    saved_loss = 0.0\n",
        "    for images,labels in tqdm(trainloader):\n",
        "        # get inputs and labels and convert to appropriate device\n",
        "        inputs, labels = images,labels\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print stats\n",
        "        running_loss += loss.item()\n",
        "    #if i % 2000 == 1999:  # print every 2000 mini-batches\n",
        "    print('loss: %.3f' %(running_loss/len(trainloader)))\n",
        "    saved_loss = running_loss\n",
        "    epoch_losses.append(saved_loss)\n",
        "print('Training done!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9SuNflgz7vU",
        "outputId": "de6e18d9-7a02-4e40-d6b2-3c5e65956547"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3125/3125 [00:46<00:00, 67.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 2.252\n",
            "Epoch: 1\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3125/3125 [00:45<00:00, 68.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 1.978\n",
            "Epoch: 2\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3125/3125 [00:45<00:00, 68.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 1.833\n",
            "Epoch: 3\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3125/3125 [00:45<00:00, 68.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 1.729\n",
            "Epoch: 4\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3125/3125 [00:46<00:00, 67.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 1.661\n",
            "Epoch: 5\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3125/3125 [00:46<00:00, 67.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 1.607\n",
            "Epoch: 6\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3125/3125 [00:46<00:00, 67.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 1.561\n",
            "Epoch: 7\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3125/3125 [00:45<00:00, 67.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 1.529\n",
            "Epoch: 8\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3125/3125 [00:45<00:00, 68.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 1.505\n",
            "Epoch: 9\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3125/3125 [00:46<00:00, 67.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 1.478\n",
            "Training done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "net.eval()\n",
        "#test_loss = []\n",
        "correct = 0\n",
        "total = 0\n",
        "running_loss = 0.0\n",
        "for images,labels in tqdm(testloader):\n",
        "        # get inputs and labels and convert to appropriate device\n",
        "        inputs, labels =  images.to(device),labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        # optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # loss.backward()\n",
        "        # optimizer.step()\n",
        "\n",
        "        # print stats\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    #if i % 2000 == 1999:  # print every 2000 mini-batches\n",
        "print('loss: %.3f' %(running_loss/len(testloader)))\n",
        "print('test done!')\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NX_8oJFv1WTK",
        "outputId": "9367d209-5892-49de-a185-271402a133ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:06<00:00, 95.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 1.433\n",
            "test done!\n",
            "Accuracy of the network on the 10000 test images: 48 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "epochs = range(1,11)\n",
        "plt.plot(epochs, epoch_losses, 'g', label='Training loss')\n",
        "plt.title('Trainingloss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "c-w_pTgW1XFo",
        "outputId": "fa1c09b0-20a3-42a9-d288-bdf849285c49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "epoch_losses = [] # using this to record the training loss so that we can plot it against the epoch\n",
        "net.train()\n",
        "for epoch in range(10):\n",
        "    print(\"Epoch:\", epoch)\n",
        "    print(\"-\"*100)\n",
        "    running_loss = 0.0\n",
        "    saved_loss = 0.0\n",
        "\n",
        "    for images,labels in tqdm(trainloader):\n",
        "        # get inputs and labels and convert to appropriate device\n",
        "        inputs, labels = images,labels\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print stats\n",
        "        running_loss += loss.item()\n",
        "    #if i % 2000 == 1999:  # print every 2000 mini-batches\n",
        "    print('loss: %.3f' %(running_loss/len(trainloader)))\n",
        "    saved_loss = running_loss\n",
        "    epoch_losses.append(saved_loss)\n",
        "print('Training done!')"
      ],
      "metadata": {
        "id": "H7R_XGa-XJTv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}